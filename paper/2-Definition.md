# Definition

According to [@IBM] the term Edge Computing refers to:

> Edge computing is a distributed computing framework that brings 
enterprise applications closer to data sources such as IoT devices 
or local edge servers. This proximity to data at its source can 
deliver strong business benefits, including faster insights,
improved response times and better bandwidth availability."

In this way, it is understood that applying edge computing results 
in superior manageability and response times, being beneficial for 
structures where scability is a determining point when applying 
different technologies, thus allowing to reduce resource consumption.

On the other hand, the rise of the Internet of Things and the 4.0 
industry are key determinants that edge computing is continuing to 
grow steadily, while the exponential growth of connected devices 
around the world is a problem for today's infrastructures. Since, 
managing this large number of linked devices and their data 
is a complex and heavy task, this is where Edge Computing offers 
a solution to support the efficiency of these systems that are 
responsible for centralizing and processing data. With Edge Computing, 
data is processed and analyzed closer to where it was created, because 
that means data doesn't have to travel to the cloud or a data center 
to be processed. Another benefit is the fact that the latency when 
working with these systems is significantly reduced [@IBM].

![[edge computing](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fjelvix.com%2Fwp-content%2Fuploads%2F2020%2F03%2Fwhat-is-edge-computing.jpg&f=1&nofb=1)](images/edge_computing1.jpg){#fig:edge_computing}
